{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "\tPredicting...\n",
      "\tRefining by pygco...\n",
      "Dx x Dy x Dz is 146.0 x 125.0 x 73.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from dl_models import *\n",
    "from sklearn import neighbors\n",
    "from loss_func import *\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pygco import cut_from_graph\n",
    "import open3d as o3d\n",
    "import json\n",
    "\n",
    "def clone_runoob(li1):\n",
    "    \"\"\"\n",
    "    copy list\n",
    "    \"\"\"\n",
    "    li_copy = li1[:]\n",
    "\n",
    "    return li_copy\n",
    "\n",
    "# Reclassify outliers\n",
    "def class_inlier_outlier(label_list, mean_points,cloud, ind, label_index, points, labels):\n",
    "    label_change = clone_runoob(labels)\n",
    "    outlier_index = clone_runoob(label_index)\n",
    "    ind_reverse = clone_runoob(ind)\n",
    "\n",
    "    # Get the label subscript of the outlier point\n",
    "    ind_reverse.reverse()\n",
    "    for i in ind_reverse:\n",
    "        outlier_index.pop(i)\n",
    "\n",
    "    # Get outliers\n",
    "    inlier_cloud = cloud.select_by_index(ind)\n",
    "    outlier_cloud = cloud.select_by_index(ind, invert=True)\n",
    "    outlier_points = np.array(outlier_cloud.points)\n",
    "\n",
    "    for i in range(len(outlier_points)):\n",
    "        distance = []\n",
    "        for j in range(len(mean_points)):\n",
    "            dis = np.linalg.norm(outlier_points[i] - mean_points[j], ord=2)  # Compute the distance between tooth and GT centroid\n",
    "            distance.append(dis)\n",
    "        min_index = distance.index(min(distance))  # Get the index of the label closest to the centroid of the outlier point\n",
    "        outlier_label = label_list[min_index]  # Get the label of the outlier point\n",
    "        index = outlier_index[i]\n",
    "        label_change[index] = outlier_label\n",
    "\n",
    "    return label_change\n",
    "\n",
    "# Use knn algorithm to eliminate outliers\n",
    "def remove_outlier(points, labels):\n",
    "    same_label_points = {}\n",
    "\n",
    "    same_label_index = {}\n",
    "\n",
    "    mean_points = [] # All label types correspond to the centroid coordinates of the point cloud.\n",
    "\n",
    "    label_list = []\n",
    "    for i in range(len(labels)):\n",
    "        label_list.append(labels[i])\n",
    "    label_list = list(set(label_list)) # To retrieve the order from small to large, take GT_label=[0, 11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25, 26, 27]\n",
    "    label_list.sort()\n",
    "    label_list = label_list[1:]\n",
    "\n",
    "    for i in label_list:\n",
    "        key = i\n",
    "        points_list = []\n",
    "        all_label_index = []\n",
    "        for j in range(len(labels)):\n",
    "            if labels[j] == i:\n",
    "                points_list.append(points[j].tolist())\n",
    "                all_label_index.append(j) # Get the subscript of the label corresponding to the point with label i\n",
    "        same_label_points[key] = points_list\n",
    "        same_label_index[key] = all_label_index\n",
    "\n",
    "        tooth_mean = np.mean(points_list, axis=0)\n",
    "        mean_points.append(tooth_mean)\n",
    "        # print(mean_points)\n",
    "\n",
    "    for i in label_list:\n",
    "        points_array = same_label_points[i]\n",
    "        # Build one o3d object\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        # UseVector3dVector conversion method\n",
    "        pcd.points = o3d.utility.Vector3dVector(points_array)\n",
    "\n",
    "        # Perform statistical outlier removal on the point cloud corresponding to label i, find outliers and display them\n",
    "        # Statistical outlier removal\n",
    "        cl, ind = pcd.remove_statistical_outlier(nb_neighbors=200, std_ratio=2.0)  # cl是选中的点，ind是选中点index\n",
    "\n",
    "        # Reclassify the separated outliers\n",
    "        label_index = same_label_index[i]\n",
    "        labels = class_inlier_outlier(label_list, mean_points, pcd, ind, label_index, points, labels)\n",
    "        # print(f\"label_change{labels[4400]}\")\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Eliminate outliers and save the final output\n",
    "def remove_outlier_main(jaw, pcd_points, labels, instances_labels):\n",
    "    # original point\n",
    "    points = pcd_points.copy()\n",
    "    label = remove_outlier(points, labels)\n",
    "\n",
    "    # Save json file\n",
    "    label_dict = {}\n",
    "    label_dict[\"id_patient\"] = \"\"\n",
    "    label_dict[\"jaw\"] = jaw\n",
    "    label_dict[\"labels\"] = label.tolist()\n",
    "    label_dict[\"instances\"] = instances_labels.tolist()\n",
    "\n",
    "    b = json.dumps(label_dict)\n",
    "    with open('dental-labels4' + '.json', 'w') as f_obj:\n",
    "        f_obj.write(b)\n",
    "    f_obj.close()\n",
    "\n",
    "same_points_list = {}\n",
    "\n",
    "# voxel downsampling\n",
    "def voxel_filter(point_cloud, leaf_size):\n",
    "    same_points_list = {}\n",
    "    filtered_points = []\n",
    "\n",
    "    # step1 Calculate boundary points\n",
    "    x_max, y_max, z_max = np.amax(point_cloud, axis=0)  # 计算 x,y,z三个维度的最值\n",
    "    x_min, y_min, z_min = np.amin(point_cloud, axis=0)\n",
    "\n",
    "    # step2 Determine the size of the voxel\n",
    "    size_r = leaf_size\n",
    "\n",
    "    # step3 Calculate the dimensions of each volex voxel grid\n",
    "    Dx = (x_max - x_min) // size_r + 1\n",
    "    Dy = (y_max - y_min) // size_r + 1\n",
    "    Dz = (z_max - z_min) // size_r + 1\n",
    "\n",
    "    # print(\"Dx x Dy x Dz is {} x {} x {}\".format(Dx, Dy, Dz))\n",
    "\n",
    "    # step4 Calculate the value of each point in each dimension in the volex grid\n",
    "    h = list()  # h is a list of saved indexes\n",
    "    for i in range(len(point_cloud)):\n",
    "        hx = np.floor((point_cloud[i][0] - x_min) // size_r)\n",
    "        hy = np.floor((point_cloud[i][1] - y_min) // size_r)\n",
    "        hz = np.floor((point_cloud[i][2] - z_min) // size_r)\n",
    "        h.append(hx + hy * Dx + hz * Dx * Dy)\n",
    "\n",
    "    # step5 Sort h values\n",
    "    h = np.array(h)\n",
    "    h_indice = np.argsort(h)  # Extract the index and return the index of the elements in h sorted from small to large.\n",
    "    h_sorted = h[h_indice]  # Ascending order\n",
    "    count = 0  # used for accumulation of dimensions\n",
    "    step = 20\n",
    "\n",
    "    # Put points with the same h value into the same grid and filter them\n",
    "    for i in range(1, len(h_sorted)):  # 0-19999 data points\n",
    "        if h_sorted[i] == h_sorted[i - 1] and (i != len(h_sorted) - 1):\n",
    "            continue\n",
    "\n",
    "        elif h_sorted[i] == h_sorted[i - 1] and (i == len(h_sorted) - 1):\n",
    "            point_idx = h_indice[count:]\n",
    "            key = h_sorted[i - 1]\n",
    "            same_points_list[key] = point_idx\n",
    "            _G = np.mean(point_cloud[point_idx], axis=0)  # center of gravity of all points\n",
    "            _d = np.linalg.norm(point_cloud[point_idx] - _G, axis=1, ord=2)  # Calculate distance to center of gravity\n",
    "            _d.sort()\n",
    "            inx = [j for j in range(0, len(_d), step)]  # Get the index of the specified interval element\n",
    "            for j in inx:\n",
    "                index = point_idx[j]\n",
    "                filtered_points.append(point_cloud[index])\n",
    "            count = i\n",
    "\n",
    "        elif h_sorted[i] != h_sorted[i - 1] and (i == len(h_sorted) - 1):\n",
    "            point_idx1 = h_indice[count:i]\n",
    "            key1 = h_sorted[i - 1]\n",
    "            same_points_list[key1] = point_idx1\n",
    "            _G = np.mean(point_cloud[point_idx1], axis=0)  # center of gravity of all points\n",
    "            _d = np.linalg.norm(point_cloud[point_idx1] - _G, axis=1, ord=2)  # Calculate distance to center of gravity\n",
    "            _d.sort()\n",
    "            inx = [j for j in range(0, len(_d), step)]  # Get the index of the specified interval element\n",
    "            for j in inx:\n",
    "                index = point_idx1[j]\n",
    "                filtered_points.append(point_cloud[index])\n",
    "\n",
    "            point_idx2 = h_indice[i:]\n",
    "            key2 = h_sorted[i]\n",
    "            same_points_list[key2] = point_idx2\n",
    "            _G = np.mean(point_cloud[point_idx2], axis=0)  # center of gravity of all points\n",
    "            _d = np.linalg.norm(point_cloud[point_idx2] - _G, axis=1, ord=2)  # Calculate distance to center of gravity\n",
    "            _d.sort()\n",
    "            inx = [j for j in range(0, len(_d), step)]  # Get the index of the specified interval element\n",
    "            for j in inx:\n",
    "                index = point_idx2[j]\n",
    "                filtered_points.append(point_cloud[index])\n",
    "            count = i\n",
    "\n",
    "        else:\n",
    "            point_idx = h_indice[count: i]\n",
    "            key = h_sorted[i - 1]\n",
    "            same_points_list[key] = point_idx\n",
    "            _G = np.mean(point_cloud[point_idx], axis=0)  # center of gravity of all points\n",
    "            _d = np.linalg.norm(point_cloud[point_idx] - _G, axis=1, ord=2)  # Calculate distance to center of gravity\n",
    "            _d.sort()\n",
    "            inx = [j for j in range(0, len(_d), step)]  # Get the index of the specified interval element\n",
    "            for j in inx:\n",
    "                index = point_idx[j]\n",
    "                filtered_points.append(point_cloud[index])\n",
    "            count = i\n",
    "\n",
    "    # Change the point cloud format to array and return it externally\n",
    "    # print(f'filtered_points[0]为{filtered_points[0]}')\n",
    "    filtered_points = np.array(filtered_points, dtype=np.float64)\n",
    "\n",
    "    return filtered_points,same_points_list\n",
    "\n",
    "# voxel upsampling\n",
    "def voxel_upsample(same_points_list, point_cloud, filtered_points, filter_labels, leaf_size):\n",
    "    upsample_label = []\n",
    "    upsample_point = []\n",
    "    upsample_index = []\n",
    "\n",
    "    # step1 Calculate boundary points\n",
    "    x_max, y_max, z_max = np.amax(point_cloud, axis=0) # Calculate the maximum value of the three dimensions x, y, z\n",
    "    x_min, y_min, z_min = np.amin(point_cloud, axis=0)\n",
    "\n",
    "    # step2 Determine the size of the voxel\n",
    "    size_r = leaf_size\n",
    "\n",
    "    # step3 Calculate the dimensions of each volex voxel grid\n",
    "    Dx = (x_max - x_min) // size_r + 1\n",
    "    Dy = (y_max - y_min) // size_r + 1\n",
    "    Dz = (z_max - z_min) // size_r + 1\n",
    "    print(\"Dx x Dy x Dz is {} x {} x {}\".format(Dx, Dy, Dz))\n",
    "\n",
    "    # step4 Calculate the value of each point (sampled point) in each dimension within the volex grid\n",
    "    h = list()\n",
    "    for i in range(len(filtered_points)):\n",
    "        hx = np.floor((filtered_points[i][0] - x_min) // size_r)\n",
    "        hy = np.floor((filtered_points[i][1] - y_min) // size_r)\n",
    "        hz = np.floor((filtered_points[i][2] - z_min) // size_r)\n",
    "        h.append(hx + hy * Dx + hz * Dx * Dy)\n",
    "\n",
    "    # step5 Query the dictionary same_points_list based on the h value\n",
    "    h = np.array(h)\n",
    "    count = 0\n",
    "    for i in range(1, len(h)):\n",
    "        if h[i] == h[i - 1] and i != (len(h) - 1):\n",
    "            continue\n",
    "\n",
    "        elif h[i] == h[i - 1] and i == (len(h) - 1):\n",
    "            label = filter_labels[count:]\n",
    "            key = h[i - 1]\n",
    "            count = i\n",
    "\n",
    "            # Cumulative number of labels, classcount: {‘A’: 2, ‘B’: 1}\n",
    "            classcount = {}\n",
    "            for i in range(len(label)):\n",
    "                vote = label[i]\n",
    "                classcount[vote] = classcount.get(vote, 0) + 1\n",
    "\n",
    "            # Sort map values\n",
    "            sortedclass = sorted(classcount.items(), key=lambda x: (x[1]), reverse=True)\n",
    "            point_index = same_points_list[key]  # Point index list corresponding to h\n",
    "            for j in range(len(point_index)):\n",
    "                upsample_label.append(sortedclass[0][0])\n",
    "                index = point_index[j]\n",
    "                upsample_point.append(point_cloud[index])\n",
    "                upsample_index.append(index)\n",
    "\n",
    "        elif h[i] != h[i - 1] and (i == len(h) - 1):\n",
    "            label1 = filter_labels[count:i]\n",
    "            key1 = h[i - 1]\n",
    "            label2 = filter_labels[i:]\n",
    "            key2 = h[i]\n",
    "            count = i\n",
    "\n",
    "            classcount = {}\n",
    "            for i in range(len(label1)):\n",
    "                vote = label1[i]\n",
    "                classcount[vote] = classcount.get(vote, 0) + 1\n",
    "\n",
    "            sortedclass = sorted(classcount.items(), key=lambda x: (x[1]), reverse=True)\n",
    "            point_index = same_points_list[key1]\n",
    "            for j in range(len(point_index)):\n",
    "                upsample_label.append(sortedclass[0][0])\n",
    "                index = point_index[j]\n",
    "                upsample_point.append(point_cloud[index])\n",
    "                upsample_index.append(index)\n",
    "\n",
    "            classcount = {}\n",
    "            for i in range(len(label2)):\n",
    "                vote = label2[i]\n",
    "                classcount[vote] = classcount.get(vote, 0) + 1\n",
    "\n",
    "            sortedclass = sorted(classcount.items(), key=lambda x: (x[1]), reverse=True)\n",
    "            point_index = same_points_list[key2]\n",
    "            for j in range(len(point_index)):\n",
    "                upsample_label.append(sortedclass[0][0])\n",
    "                index = point_index[j]\n",
    "                upsample_point.append(point_cloud[index])\n",
    "                upsample_index.append(index)\n",
    "        else:\n",
    "            label = filter_labels[count:i]\n",
    "            key = h[i - 1]\n",
    "            count = i\n",
    "            classcount = {}\n",
    "            for i in range(len(label)):\n",
    "                vote = label[i]\n",
    "                classcount[vote] = classcount.get(vote, 0) + 1\n",
    "\n",
    "            sortedclass = sorted(classcount.items(), key=lambda x: (x[1]), reverse=True)\n",
    "            point_index = same_points_list[key]  # h对应的point index列表\n",
    "            for j in range(len(point_index)):\n",
    "                upsample_label.append(sortedclass[0][0])\n",
    "                index = point_index[j]\n",
    "                upsample_point.append(point_cloud[index])\n",
    "                upsample_index.append(index)\n",
    "\n",
    "    # Restore the original order of index\n",
    "    upsample_index = np.array(upsample_index)\n",
    "    upsample_index_indice = np.argsort(upsample_index) # Extract the index and return the index of the elements in h sorted from small to large.\n",
    "    upsample_index_sorted = upsample_index[upsample_index_indice]\n",
    "\n",
    "    upsample_point = np.array(upsample_point)\n",
    "    upsample_label = np.array(upsample_label)\n",
    "\n",
    "    # Restore the original order of points and labels\n",
    "    upsample_point_sorted = upsample_point[upsample_index_indice]\n",
    "    upsample_label_sorted = upsample_label[upsample_index_indice]\n",
    "\n",
    "    return upsample_point_sorted, upsample_label_sorted\n",
    "\n",
    "# Upsampling using knn algorithm\n",
    "def KNN_sklearn_Load_data(voxel_points, center_points, labels):\n",
    "    # Build model\n",
    "    model = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "    model.fit(center_points, labels)\n",
    "    prediction = model.predict(voxel_points.reshape(1, -1))\n",
    "\n",
    "    return prediction[0]\n",
    "\n",
    "# Loading points for knn upsampling\n",
    "def Load_data(voxel_points, center_points, labels):\n",
    "    meshtopoints_labels = []\n",
    "    for i in range(0, voxel_points.shape[0]):\n",
    "        meshtopoints_labels.append(KNN_sklearn_Load_data(voxel_points[i], center_points, labels))\n",
    "\n",
    "    return np.array(meshtopoints_labels)\n",
    "\n",
    "# Upsample triangular mesh data back to original point cloud data\n",
    "def mesh_to_points_main(jaw, pcd_points, center_points, labels):\n",
    "    points = pcd_points.copy()\n",
    "\n",
    "    # Downsampling\n",
    "    voxel_points, same_points_list = voxel_filter(points, 0.6)\n",
    "\n",
    "    after_labels = Load_data(voxel_points, center_points, labels)\n",
    "\n",
    "    upsample_point, upsample_label = voxel_upsample(same_points_list, points, voxel_points, after_labels, 0.6)\n",
    "\n",
    "    new_pcd = o3d.geometry.PointCloud()\n",
    "    new_pcd.points = o3d.utility.Vector3dVector(upsample_point)\n",
    "    instances_labels = upsample_label.copy()\n",
    "\n",
    "    # Reclassify the label of the upper and lower jaws\n",
    "    for i in range(0, upsample_label.shape[0]):\n",
    "        if jaw == 'upper':\n",
    "            if (upsample_label[i] >= 1) and (upsample_label[i] <= 8):\n",
    "                upsample_label[i] = upsample_label[i] + 10\n",
    "            elif (upsample_label[i] >= 9) and (upsample_label[i] <= 16):\n",
    "                upsample_label[i] = upsample_label[i] + 12\n",
    "        else:\n",
    "            if (upsample_label[i] >= 1) and (upsample_label[i] <= 8):\n",
    "                upsample_label[i] = upsample_label[i] + 30\n",
    "            elif (upsample_label[i] >= 9) and (upsample_label[i] <= 16):\n",
    "                upsample_label[i] = upsample_label[i] + 32\n",
    "\n",
    "    remove_outlier_main(jaw, pcd_points, upsample_label, instances_labels)\n",
    "\n",
    "# Convert raw point cloud data to triangular mesh\n",
    "def mesh_grid(pcd_points):\n",
    "    new_pcd,_ = voxel_filter(pcd_points, 0.6)\n",
    "    # pcd needs to have a normal vector\n",
    "\n",
    "    # estimate radius for rolling ball\n",
    "    pcd_new = o3d.geometry.PointCloud()\n",
    "    pcd_new.points = o3d.utility.Vector3dVector(new_pcd)\n",
    "    pcd_new.estimate_normals()\n",
    "    distances = pcd_new.compute_nearest_neighbor_distance()\n",
    "    avg_dist = np.mean(distances)\n",
    "    radius = 6 * avg_dist\n",
    "    mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(\n",
    "        pcd_new,\n",
    "        o3d.utility.DoubleVector([radius, radius * 2]))\n",
    "\n",
    "    return mesh\n",
    "\n",
    "# Read the contents of obj file\n",
    "def read_obj(obj_path):\n",
    "    jaw = None\n",
    "    with open(obj_path) as file:\n",
    "        points = []\n",
    "        faces = []\n",
    "        while 1:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            strs = line.split(\" \")\n",
    "            if strs[0] == \"v\":\n",
    "                points.append((float(strs[1]), float(strs[2]), float(strs[3])))\n",
    "            elif strs[0] == \"f\":\n",
    "                faces.append((int(strs[1]), int(strs[2]), int(strs[3])))\n",
    "            elif strs[1][0:5] == 'lower':\n",
    "                jaw = 'lower'\n",
    "            elif strs[1][0:5] == 'upper':\n",
    "                jaw = 'upper'\n",
    "\n",
    "    points = np.array(points)\n",
    "    faces = np.array(faces)\n",
    "    if jaw is None:\n",
    "        raise ValueError(\"Jaw type not found in OBJ file\")\n",
    "\n",
    "    return points, faces, jaw\n",
    "\n",
    "# Convert obj file to pcd file\n",
    "def obj2pcd(obj_path):\n",
    "    if os.path.exists(obj_path):\n",
    "        print('yes')\n",
    "    points, _, jaw = read_obj(obj_path)\n",
    "    pcd_list = []\n",
    "    num_points = np.shape(points)[0]\n",
    "    for i in range(num_points):\n",
    "        new_line = str(points[i, 0]) + ' ' + str(points[i, 1]) + ' ' + str(points[i, 2])\n",
    "        pcd_list.append(new_line.split())\n",
    "\n",
    "    pcd_points = np.array(pcd_list).astype(np.float64)\n",
    "\n",
    "    return pcd_points, jaw\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    obj_path = \"input/.obj\"\n",
    "    # ground_truth_pth = 'ground_truth/'\n",
    "    save_path = 'output/'\n",
    "    # gpu_id = utils.get_avail_gpu()\n",
    "    # gpu_id = 0\n",
    "    # torch.cuda.set_device(gpu_id) # assign which gpu will be used (only linux works)\n",
    "\n",
    "    upsampling_method = 'KNN'\n",
    "\n",
    "    model_path = 'model.tar'\n",
    "    num_classes = 15\n",
    "    num_channels = 15\n",
    "\n",
    "    # set model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MeshSegNet(num_classes=num_classes, num_channels=num_channels).to(device, dtype=torch.float)\n",
    "\n",
    "    # load trained model\n",
    "    # checkpoint = torch.load(os.path.join(model_path, model_name), map_location='cpu')\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    del checkpoint\n",
    "    model = model.to(device, dtype=torch.float)\n",
    "\n",
    "    # cudnn\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "    # Predicting\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pcd_points, jaw = obj2pcd(obj_path)\n",
    "        mesh = mesh_grid(pcd_points)\n",
    "        pcd_points, jaw = obj2pcd(obj_path)\n",
    "        mesh = mesh_grid(pcd_points)\n",
    "\n",
    "        # move mesh to origin\n",
    "        print('\\tPredicting...')\n",
    "\n",
    "        vertices_points = np.asarray(mesh.vertices)\n",
    "        triangles_points = np.asarray(mesh.triangles)\n",
    "        N = triangles_points.shape[0]\n",
    "        cells = np.zeros((triangles_points.shape[0], 9))\n",
    "        cells = vertices_points[triangles_points].reshape(triangles_points.shape[0], 9)\n",
    "\n",
    "        mean_cell_centers = mesh.get_center()\n",
    "        cells[:, 0:3] -= mean_cell_centers[0:3]\n",
    "        cells[:, 3:6] -= mean_cell_centers[0:3]\n",
    "        cells[:, 6:9] -= mean_cell_centers[0:3]\n",
    "\n",
    "        v1 = np.zeros([triangles_points.shape[0], 3], dtype='float32')\n",
    "        v2 = np.zeros([triangles_points.shape[0], 3], dtype='float32')\n",
    "        v1[:, 0] = cells[:, 0] - cells[:, 3]\n",
    "        v1[:, 1] = cells[:, 1] - cells[:, 4]\n",
    "        v1[:, 2] = cells[:, 2] - cells[:, 5]\n",
    "        v2[:, 0] = cells[:, 3] - cells[:, 6]\n",
    "        v2[:, 1] = cells[:, 4] - cells[:, 7]\n",
    "        v2[:, 2] = cells[:, 5] - cells[:, 8]\n",
    "        mesh_normals = np.cross(v1, v2)\n",
    "        mesh_normal_length = np.linalg.norm(mesh_normals, axis=1)\n",
    "        mesh_normals[:, 0] /= mesh_normal_length[:]\n",
    "        mesh_normals[:, 1] /= mesh_normal_length[:]\n",
    "        mesh_normals[:, 2] /= mesh_normal_length[:]\n",
    "\n",
    "        # prepare input\n",
    "        # points = mesh.points().copy()\n",
    "        points = vertices_points.copy()\n",
    "        points[:, 0:3] -= mean_cell_centers[0:3]\n",
    "        normals = np.nan_to_num(mesh_normals).copy()\n",
    "        barycenters = np.zeros((triangles_points.shape[0], 3))\n",
    "        s = np.sum(vertices_points[triangles_points], 1)\n",
    "        barycenters = 1 / 3 * s\n",
    "        center_points = barycenters.copy()\n",
    "        # np.save(os.path.join(output_path, name + '.npy'), barycenters)\n",
    "        barycenters -= mean_cell_centers[0:3]\n",
    "\n",
    "        # normalized data\n",
    "        maxs = points.max(axis=0)\n",
    "        mins = points.min(axis=0)\n",
    "        means = points.mean(axis=0)\n",
    "        stds = points.std(axis=0)\n",
    "        nmeans = normals.mean(axis=0)\n",
    "        nstds = normals.std(axis=0)\n",
    "\n",
    "        for i in range(3):\n",
    "            cells[:, i] = (cells[:, i] - means[i]) / stds[i]  # point 1\n",
    "            cells[:, i + 3] = (cells[:, i + 3] - means[i]) / stds[i]  # point 2\n",
    "            cells[:, i + 6] = (cells[:, i + 6] - means[i]) / stds[i]  # point 3\n",
    "            barycenters[:, i] = (barycenters[:, i] - mins[i]) / (maxs[i] - mins[i])\n",
    "            normals[:, i] = (normals[:, i] - nmeans[i]) / nstds[i]\n",
    "\n",
    "        X = np.column_stack((cells, barycenters, normals))\n",
    "\n",
    "        # computing A_S and A_L\n",
    "        A_S = np.zeros([X.shape[0], X.shape[0]], dtype='float32')\n",
    "        A_L = np.zeros([X.shape[0], X.shape[0]], dtype='float32')\n",
    "        D = distance_matrix(X[:, 9:12], X[:, 9:12])\n",
    "        A_S[D < 0.1] = 1.0\n",
    "        A_S = A_S / np.dot(np.sum(A_S, axis=1, keepdims=True), np.ones((1, X.shape[0])))\n",
    "\n",
    "        A_L[D < 0.2] = 1.0\n",
    "        A_L = A_L / np.dot(np.sum(A_L, axis=1, keepdims=True), np.ones((1, X.shape[0])))\n",
    "\n",
    "        # numpy -> torch.tensor\n",
    "        X = X.transpose(1, 0)\n",
    "        X = X.reshape([1, X.shape[0], X.shape[1]])\n",
    "        X = torch.from_numpy(X).to(device, dtype=torch.float)\n",
    "        A_S = A_S.reshape([1, A_S.shape[0], A_S.shape[1]])\n",
    "        A_L = A_L.reshape([1, A_L.shape[0], A_L.shape[1]])\n",
    "        A_S = torch.from_numpy(A_S).to(device, dtype=torch.float)\n",
    "        A_L = torch.from_numpy(A_L).to(device, dtype=torch.float)\n",
    "\n",
    "        tensor_prob_output = model(X, A_S, A_L).to(device, dtype=torch.float)\n",
    "        patch_prob_output = tensor_prob_output.cpu().numpy()\n",
    "\n",
    "        round_factor = 100\n",
    "        patch_prob_output[patch_prob_output < 1.0e-6] = 1.0e-6\n",
    "\n",
    "        # unaries\n",
    "        unaries = -round_factor * np.log10(patch_prob_output)\n",
    "        unaries = unaries.astype(np.int32)\n",
    "        unaries = unaries.reshape(-1, num_classes)\n",
    "\n",
    "        # parawisex\n",
    "        pairwise = (1 - np.eye(num_classes, dtype=np.int32))\n",
    "\n",
    "        cells = cells.copy()\n",
    "\n",
    "        cell_ids = np.asarray(triangles_points)\n",
    "        lambda_c = 20\n",
    "        edges = np.empty([1, 3], order='C')\n",
    "        # Find neighbors\n",
    "        for i_node in range(cells.shape[0]):\n",
    "            nei = np.sum(np.isin(cell_ids, cell_ids[i_node, :]), axis=1)\n",
    "            nei_id = np.where(nei == 2)\n",
    "            for i_nei in nei_id[0][:]:\n",
    "                if i_node < i_nei:\n",
    "                    cos_theta = np.dot(normals[i_node, 0:3], normals[i_nei, 0:3]) / np.linalg.norm(\n",
    "                        normals[i_node, 0:3]) / np.linalg.norm(normals[i_nei, 0:3])\n",
    "\n",
    "                    if cos_theta >= 1.0:\n",
    "                        cos_theta = 0.9999\n",
    "                    theta = np.arccos(cos_theta)\n",
    "                    phi = np.linalg.norm(barycenters[i_node, :] - barycenters[i_nei, :])\n",
    "                    if theta > np.pi / 2.0:\n",
    "                        edges = np.concatenate(\n",
    "                            (edges, np.array([i_node, i_nei, -np.log10(theta / np.pi) * phi]).reshape(1, 3)), axis=0)\n",
    "                    else:\n",
    "                        beta = 1 + np.linalg.norm(np.dot(normals[i_node, 0:3], normals[i_nei, 0:3]))\n",
    "                        edges = np.concatenate(\n",
    "                            (edges, np.array([i_node, i_nei, -beta * np.log10(theta / np.pi) * phi]).reshape(1, 3)),\n",
    "                            axis=0)\n",
    "\n",
    "        edges = np.delete(edges, 0, 0)\n",
    "        edges[:, 2] *= lambda_c * round_factor\n",
    "        edges = edges.astype(np.int32)\n",
    "\n",
    "        refine_labels = cut_from_graph(edges, unaries, pairwise)\n",
    "        refine_labels = refine_labels.reshape([-1, 1])\n",
    "\n",
    "        predicted_labels_3 = refine_labels.reshape(refine_labels.shape[0])\n",
    "        mesh_to_points_main(jaw, pcd_points, center_points, predicted_labels_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
